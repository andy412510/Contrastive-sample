# general
start_epoch: 0
epoch: 20
stage: 3
height: 256                      # input image height
width: 128                       # input image width
batch_size: 16                    # batch size
loss_freq: 100                # How many iter. do you want to plot loss
init: kaiming                  # initialization [gaussian/kaiming/xavier/orthogonal]
snapshot_save_iter: 2000       # How often to save the checkpoint
# path 
E_c_init: /home/andy/ICASSP_data/pretrain/JVTC/pretrain.pth
checkpoint_path: /home/andy/ICASSP_data/checkpoint/
output_path: ./outputs/

# lr
lr_id: 0.00035                    # initial appearance encoder learning rate
lr_d: 0.0001                     # initial discriminator learning rate
lr_g: 0.0001                     # initial generator (except appearance encoder) learning rate
beta1: 0.5                         # Adam hyperparameter
beta2: 0.999                     # Adam hyperparameter
step_size: 10                # when to decay the learning rate
weight_decay: 0.0005             # weight decay
gamma: 0.1                       # Learning Rate Decay 
lr_policy: multistep             # learning rate scheduler [constant|step]
# loss
gan_w: 1                      # weight of adversarial loss
recon_x_w: 5                 # weight of image reconstruction loss
recon_s_w: 1                  # weight of style reconstruction loss
recon_c_w: 1                  # weight of content reconstruction loss
recon_x_cyc_w: 5              # weight of explicit style augmented cycle consistency loss
contrastive_w: 1                  # contrastive loss
# memory
momentum: 0.2                     # memory momentum alpha
temperature: 0.07                 # contrastive temperature
K: 8192                           # number of negative samples
num_samples: 12936                # market = 12936, duke = 16522
# model
input_dim: 3
pose_dim: 1
# cluster
eps: 0.6
k1: 30
k2: 6

dis:              
  LAMBDA: 0.01                   # the hyperparameter for the regularization term
  activ: lrelu                   # activation function style [relu/lrelu/prelu/selu/tanh]
  dim: 32                        # number of filters in the bottommost layer
  gan_type: lsgan                # GAN loss [lsgan/nsgan]
  n_layer: 2                     # number of layers in D
  n_res: 4                       # number of layers in D
  non_local: 0                   # number of non_local layers
  norm: none                     # normalization layer [none/bn/in/ln]
  num_scales: 3                  # number of scales
  pad_type: reflect              # padding type [zero/reflect]
  smooth_label: True

gen:
  activ: lrelu                   # activation function style [relu/lrelu/prelu/selu/tanh]
  dec: basic                     # [basic/parallel/series]
  dim: 16                        # number of filters in the bottommost layer
  dropout: 0                     # use dropout in the generator
  pose_feature_nc: 128           # length of feature vector for pose
  id_dim: 2048                   # length of appearance code
  mlp_dim: 512                   # number of filters in MLP
  mlp_norm: none                 # norm in mlp [none/bn/in/ln]
  n_downsample: 2                # number of downsampling layers in content encoder
  n_res: 4                       # number of residual blocks in content encoder/decoder
  non_local: 0                   # number of non_local layer
  pad_type: reflect              # padding type [zero/reflect]
  tanh: false                    # use tanh or not at the last layer
  init: kaiming                  # initialization [gaussian/kaiming/xavier/orthogonal]


